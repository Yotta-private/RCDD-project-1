{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import theano.tensor as T \n",
    "import theano\n",
    "import keras\n",
    "from keras import activations, initializers, regularizers, Sequential\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.engine.topology import Layer\n",
    "from neural_fp import Graph, molToGraph\n",
    "import rdkit\n",
    "from rdkit import Chem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GraphFP(Layer):\n",
    "    \n",
    "    #嵌入层将正整数（下标）转换为具有固定大小的向量，如[[4],[20]]->[[0.25,0.1],[0.6,-0.2]] Embedding层只能作为模型的第一层\n",
    "    \n",
    "    \"\"\"Embedding layer for undirected, attributed graphs following the ideas of the extended connectivity fingerprints (ECFPs) or functional connectivity FPs (FCFPs).\n",
    "    It should be the first layer in a model.\n",
    "    \n",
    "    # Input shape\n",
    "        4D array with shape:(n_samples, n_atoms, n_atoms, n_features)`.\n",
    "        \n",
    "    # Output shape\n",
    "        2D tensor with shape:(n_samples, output_dim)`.\n",
    "        \n",
    "    # Arguments\n",
    "        output_dim: int > 0, size of the fingerprint\n",
    "        \n",
    "        inner_dim: (32+8-1)the number of attributes for each (bond, atom) pair concatenated. Does NOT include the extract is_bond_present flag.\n",
    "        \n",
    "        depth: radius of fingerprint (how many times to recursively mix attributes)\n",
    "        \n",
    "        init_output: initialization for weights in output layer\n",
    "        \n",
    "        activation_output: activation function for output layer. Softmax is recommended because it can help increase sparsity, making it more like a real fingerprint\n",
    "        init_inner: initialization for inner weights for mixing attributes. Identity is recommended for the initialization for simplicity\n",
    "        \n",
    "        activation_inner: activation function for the inner layer.\n",
    "        \n",
    "        scale_output: scale to use for output weight initializations. Large output weights are closer to a true sparse fingerprint, but small output weights might\n",
    "            be better to not get stuck in local minima (with low gradients)\n",
    "            \n",
    "        padding: whether to look for padding in the input tensors. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim, inner_dim, depth = 2, init_output='uniform', activation_output='softmax', init_inner='identity',\n",
    "            activation_inner='linear', scale_output=0.01, padding=False, **kwargs): # **kwargs表示关键字参数，它是一个dict\n",
    "        \n",
    "        if depth < 1:\n",
    "            quit('Cannot use GraphFP with depth zero')\n",
    "            \n",
    "        self.init_output = initializers.get(init_output)\n",
    "        self.activation_output = activations.get(activation_output)\n",
    "        self.init_inner = initializers.get(init_inner)\n",
    "        self.activation_inner = activations.get(activation_inner)\n",
    "        self.output_dim = output_dim\n",
    "        self.inner_dim = inner_dim\n",
    "        self.depth = depth\n",
    "        self.scale_output = scale_output\n",
    "        self.padding = padding\n",
    "\n",
    "        self.initial_weights = None\n",
    "        self.input_dim = 4 # each entry is a 3D N_atom x N_atom x N_feature tensor\n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (None, None, None,) # 3D tensor for each input\n",
    "        #self.input = K.placeholder(ndim = 4)\n",
    "        super(GraphFP, self).__init__(**kwargs)\n",
    "\n",
    "        \n",
    "        \n",
    "        #if self.input_dim:\n",
    "        #self.input_shape = input_shape #  define 3D tensor for each input # input_shape = (None, None, None,)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        '''Builds internal weights and paramer attribute'''\n",
    "        # NOTE: NEED TO TILE AND EVALUATE SO THAT PARAMS CAN BE VARIABLES 可变\n",
    "        # OTHERWISE K.GET_VALUE() DOES NOT WORK\n",
    "        # keras2.0 vs keras 1.0\n",
    "        # initializer = keras.initializers.RandomUniform(-1, 1)\n",
    "        # config = initializer.get_config()\n",
    "        # initializer = keras.initializers.RandomUniform()\n",
    "        # print(keras.initializers.RandomUniform().__call__((3, 1)).eval())\n",
    "\n",
    "        # Define template weights for inner FxF ------shape\n",
    "        \n",
    "        W_inner = keras.initializers.Identity().__call__((self.inner_dim, self.inner_dim)) #FxF\n",
    "        b_inner = keras.initializers.Zeros().__call__((1, self.inner_dim)) # 1xF\n",
    "        \n",
    "        self.W_inner = K.variable(T.tile(W_inner, (self.depth + 1, 1, 1)).eval() + \\\n",
    "        keras.initializers.RandomUniform().__call__((self.depth + 1, self.inner_dim, self.inner_dim)).eval()) # T构造符号变量；T.tile(x, reps) 按照规则重复 x\n",
    "        self.W_inner.name = 'T:W_inner' # eval()将字符串str当成有效的表达式来求值并返回计算结果\n",
    "        self.b_inner = K.variable(T.tile(b_inner, (self.depth + 1, 1, 1)).eval()  + \\\n",
    "        keras.initializers.RandomUniform().__call__((self.depth + 1, 1, self.inner_dim)).eval())\n",
    "        self.b_inner.name = 'T:b_inner'\n",
    "        \n",
    "        #print(self.W_inner.get_value())\n",
    "        #print( self.b_inner.get_value())\n",
    "\n",
    "        # # Concatenate third dimension (depth) so different layers can have \n",
    "        # # different weights. Now, self.W_inner[#,:,:] corresponds to the \n",
    "        # # weight matrix for layer/depth #.\n",
    "\n",
    "        # Define template weights for output FxL\n",
    "        \n",
    "        W_output = keras.initializers.VarianceScaling(scale=self.scale_output, distribution='uniform').__call__((self.inner_dim, self.output_dim)) # scale_output=0.01\n",
    "        b_output = keras.initializers.Zeros().__call__((1, self.output_dim))\n",
    "        \n",
    "        # Initialize weights tensor\n",
    "        self.W_output = K.variable(T.tile(W_output, (self.depth + 1, 1, 1)).eval())\n",
    "        self.W_output.name = 'T:W_output'\n",
    "        self.b_output = K.variable(T.tile(b_output, (self.depth + 1, 1, 1)).eval())\n",
    "        self.b_output.name = 'T:b_output'\n",
    "        \n",
    "        # # Concatenate third dimension (depth) so different layers can have \n",
    "        # # different weights. Now, self.W_output[#,:,:] corresponds to the \n",
    "        # # weight matrix for layer/depth #.\n",
    "        \n",
    "        #print(self.W_output.get_value())\n",
    "        #print(self.b_output.get_value())\n",
    "        \n",
    "        # Pack params\n",
    "        \n",
    "        self.trainable_weights = [self.W_inner, \n",
    "                       self.b_inner,\n",
    "                       self.W_output,\n",
    "                       self.b_output]\n",
    "        self.params = [self.W_inner, \n",
    "                       self.b_inner,\n",
    "                       self.W_output,\n",
    "                       self.b_output]\n",
    "        \n",
    "        super(GraphFP, self).build(input_shape)\n",
    "        #return self.trainable_weights, self.params\n",
    "    \n",
    "#W_output = [return][0][2]\n",
    "#L = GraphFP(output_dim=512, inner_dim=4, depth = 2)\n",
    "#w = GraphFP.build(L, input_shape = (4,4,40,))\n",
    "\n",
    "#c = GraphFP(output_dim=3, inner_dim=4)\n",
    "#super(GraphFP, c).build(\"input_shape\")\n",
    "#c.build(\"input_shape\")\n",
    "#c = GraphFP(output_dim=3, inner_dim=4)\n",
    "#super(GraphFP, c).build(\"input_shape\")\n",
    "#c.build(\"input_shape\")\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        (output, updates) = theano.scan(lambda x_one: self.get_output_singlesample(x_one), sequences = x)\n",
    "        return output\n",
    "\n",
    "    def get_output_singlesample(self, original_graph):\n",
    "        '''For a 3D tensor, get the output. Avoids the need for even more complicated vectorization'''\n",
    "        # Check padding\n",
    "        if self.padding:\n",
    "            rowsum = original_graph.sum(axis = 0) # add across\n",
    "            trim = rowsum[:, -1] # last feature == bond flag\n",
    "            trim_to = T.eq(trim, 0).nonzero()[0][0] # first index with no bonds\n",
    "            original_graph = original_graph[:trim_to, :trim_to, :] # reduced graph\n",
    "\n",
    "        # Get attribute values for layer zero\n",
    "        # where attributes is a 2D tensor and attributes[#, :] is the vector of\n",
    "        # concatenated node and edge attributes. In the first layer (depth 0), the \n",
    "        # edge attribute section is initialized to zeros. After increasing depth, howevevr,\n",
    "        # this part of the vector will become non-zero.\n",
    "\n",
    "        # The first attributes matrix is just graph_tensor[i, i, :], but we can't use that \n",
    "        # kind of advanced indexing\n",
    "        # Want to extract tensor diagonal as matrix, but can't do that directly...\n",
    "        # Want to loop over third dimension, so need to dimshuffle\n",
    "        (attributes, updates) = theano.scan(lambda x: x.diagonal(), sequences = original_graph.dimshuffle((2, 0, 1)))\n",
    "        attributes.name = 'attributes'\n",
    "        # Now the attributes is (N_features x N_nodes), so we need to transpose\n",
    "        attributes = attributes.T\n",
    "        attributes.name = 'attributes post-transpose'\n",
    "\n",
    "        # Get initial fingerprint\n",
    "        presum_fp = self.attributes_to_fp_contribution(attributes, 0)\n",
    "        fp = K.sum(presum_fp, axis = 0) # sum across atom contributions\n",
    "        fp.name = 'initial fingerprint'\n",
    "\n",
    "        # Get bond matrix\n",
    "        bonds = original_graph[:, :, -1] # flag if the bond is present, (N_atom x N_atom)\n",
    "        bonds.name = 'bonds'\n",
    "\n",
    "        # Iterate through different depths, updating attributes each time\n",
    "        graph = original_graph\n",
    "        for depth in range(self.depth):\n",
    "            (attributes, graph) = self.attributes_update(attributes, depth + 1, graph, original_graph, bonds)\n",
    "            presum_fp_new = self.attributes_to_fp_contribution(attributes, depth + 1)\n",
    "            presum_fp_new.name = 'presum_fp_new contribution'\n",
    "            fp = fp + K.sum(presum_fp_new, axis = 0) \n",
    "\n",
    "        return fp\n",
    "\n",
    "    def attributes_update(self, attributes, depth, graph, original_graph, bonds):\n",
    "        '''Given the current attributes, the current depth, and the graph that the attributes\n",
    "        are based on, this function will update the 2D attributes tensor'''\n",
    "\n",
    "        ############# GET NEW ATTRIBUTE MATRIX #########################\n",
    "        # New pre-activated attribute matrix v = M_i,j,: x ones((N_atom, 1)) -> (N_atom, N_features) \n",
    "        # as long as dimensions are appropriately shuffled\n",
    "        shuffled_graph = graph.copy().dimshuffle((2, 0, 1)) # (N_feature x N_atom x N_atom)\n",
    "        shuffled_graph.name = 'shuffled_graph'\n",
    "\n",
    "        ones_vec = K.ones_like(attributes[:, 0]) # (N_atom x 1)\n",
    "        ones_vec.name = 'ones_vec'\n",
    "        (new_preactivated_attributes, updates) = theano.scan(lambda x: K.dot(x, ones_vec), sequences = shuffled_graph) # (N_features x N_atom)\n",
    "\n",
    "        # Need to pass through an activation function still\n",
    "        # Final attribute = bond flag = is not part of W_inner or b_inner\n",
    "        (new_attributes, updates) = theano.scan(lambda x: self.activation_inner(\n",
    "            K.dot(x, self.W_inner[depth, :, :]) + self.b_inner[depth, 0, :]), sequences = new_preactivated_attributes[:-1, :].T) # (N_atom x N_features -1)\n",
    "\n",
    "        # Append last feature (bond flag) after the loop\n",
    "        new_attributes = K.concatenate((new_attributes, attributes[:, -1:]), axis = 1)\n",
    "        new_attributes.name = 'new_attributes'\n",
    "\n",
    "\n",
    "        ############ UPDATE GRAPH TENSOR WITH NEW ATOM ATTRIBUTES ###################\n",
    "        ### Node attribute contribution is located in every entry of graph[i,j,:] where\n",
    "        ### there is a bond @ ij or when i = j (self)\n",
    "        # Get atoms matrix (identity)\n",
    "        atoms = T.identity_like(bonds) # (N_atom x N_atom)\n",
    "        atoms.name = 'atoms_identity'\n",
    "        # Combine\n",
    "        bonds_or_atoms = bonds + atoms # (N_atom x N_atom)\n",
    "        bonds_or_atoms.name = 'bonds_or_atoms'\n",
    "\n",
    "        atom_indeces = T.arange(ones_vec.shape[0]) # 0 to N_atoms - 1 (indeces)\n",
    "        atom_indeces.name = 'atom_indeces vector'\n",
    "        ### Subtract previous node attribute contribution\n",
    "        # Multiply each entry in bonds_or_atoms by the previous atom features for that column\n",
    "        (old_features_to_sub, updates) = theano.scan(lambda i: T.outer(bonds_or_atoms[:, i], attributes[i, :]), \n",
    "            sequences = T.arange(ones_vec.shape[0]))\n",
    "        old_features_to_sub.name = 'old_features_to_sub'\n",
    "\n",
    "        ### Add new node attribute contribution\n",
    "        # Multiply each entry in bonds_or_atoms by the previous atom features for that column\n",
    "        (new_features_to_add, updates) = theano.scan(lambda i: T.outer(bonds_or_atoms[:, i], new_attributes[i, :]),\n",
    "            sequences = T.arange(ones_vec.shape[0]))\n",
    "        new_features_to_add.name = 'new_features_to_add'\n",
    "\n",
    "        # Update new graph\n",
    "        new_graph = graph - old_features_to_sub + new_features_to_add\n",
    "        new_graph.name = 'new_graph'\n",
    "\n",
    "        return (new_attributes, new_graph)\n",
    "\n",
    "\n",
    "    def attributes_to_fp_contribution(self, attributes, depth):\n",
    "        '''Given a 2D tensor of attributes where the first dimension corresponds to a single\n",
    "        node, this method will apply the output sparsifying (often softmax) function and return\n",
    "        the contribution to the fingerprint'''\n",
    "        # Apply output activation function\n",
    "        output_dot = K.dot(attributes[:, :-1], self.W_output[depth, :, :]) # ignore last attribute (bond flag)\n",
    "        output_dot.name = 'output_dot'\n",
    "        output_bias = self.b_output[depth, 0, :]\n",
    "        output_bias.name = 'output_bias'\n",
    "        output_activated = keras.activations.softmax(output_dot + output_bias)\n",
    "        output_activated.name = 'output_activated'\n",
    "        return output_activated\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'output_dim': self.output_dim,\n",
    "                  'inner_dim' : self.inner_dim,\n",
    "                  'init_output' : self.init_output.__name__,\n",
    "                  'init_inner' : self.init_inner.__name__,\n",
    "                  'activation_inner': self.activation_inner.__name__,\n",
    "                  'activation_output' : self.activation_output.__name__,\n",
    "                  'input_dim': self.input_dim,\n",
    "                  'depth' : self.depth}\n",
    "        base_config = super(GraphFP, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Are we using a convolutional embedding or a fingerprint representation?\\n# if type(use_fp) == type(None): # normal mode, use convolution\\nmodel = Sequential()\\nmodel.add(GraphFP(output_dim = 512, inner_dim = 39, depth = 2, scale_output = 0.01, padding = False, activation_inner = \\'tanh\\'))\\n\\nprint(\\'    model: added GraphFP layer ({} -> {})\\'.format(\\'mol_graph_tensor\\', \"512\"))\\n\\nmodel.add(Dense(100, activation = \"tanh\"))\\nmodel.add(Dense(50, activation = \"tanh\"))\\nmodel.add(Dense(1, activation = \"softmax\"))\\n\\nmodel.compile(optimizer=\\'rmsprop\\',\\n              loss=\\'binary_crossentropy\\',\\n              metrics=[\\'accuracy\\'])\\nmodel.summary()\\nmodel.layers'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Are we using a convolutional embedding or a fingerprint representation?\n",
    "# if type(use_fp) == type(None): # normal mode, use convolution\n",
    "model = Sequential()\n",
    "model.add(GraphFP(output_dim = 512, inner_dim = 39, depth = 2, scale_output = 0.01, padding = False, activation_inner = 'tanh'))\n",
    "\n",
    "print('    model: added GraphFP layer ({} -> {})'.format('mol_graph_tensor', \"512\"))\n",
    "\n",
    "model.add(Dense(100, activation = \"tanh\"))\n",
    "model.add(Dense(50, activation = \"tanh\"))\n",
    "model.add(Dense(1, activation = \"softmax\"))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.layers\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02451317 0.02594347 0.03470994 0.04700893 0.0412426  0.02623862\n",
      " 0.02932492 0.02792628 0.0590153  0.0262643  0.01622034 0.01885282\n",
      " 0.05180684 0.07611229 0.05297964 0.02917972 0.02929987 0.02363816\n",
      " 0.02968931 0.0453178  0.0172458  0.0425227  0.02213812 0.03754687\n",
      " 0.04031465 0.01437854 0.02954868 0.04814696 0.03212436 0.03885618\n",
      " 0.02300616 0.02340795 0.02082077 0.02013715 0.07588958 0.01862267\n",
      " 0.02363445 0.01947723 0.0270839  0.01712051 0.02973667 0.02089453\n",
      " 0.02143765 0.04326444 0.01806214 0.02865025 0.03005187 0.04575596\n",
      " 0.09950674 0.02001758 0.04009075 0.03189254 0.02685233 0.02041009\n",
      " 0.0518884  0.03718375 0.02098349 0.06078261 0.05525658 0.02359278\n",
      " 0.03923897 0.01830068 0.02871227 0.02335113 0.0225366  0.06591767\n",
      " 0.0222311  0.05555976 0.04147316 0.0429198  0.02170608 0.02033751\n",
      " 0.02139765 0.06774314 0.03517901 0.02705134 0.03441836 0.01374036\n",
      " 0.02558817 0.03481023 0.03390708 0.07334254 0.02146476 0.02674397\n",
      " 0.02236965 0.03620297 0.03632073 0.02531552 0.02613685 0.03003665\n",
      " 0.02888221 0.03862642 0.01906777 0.04544035 0.02234592 0.02002562\n",
      " 0.03923559 0.02340434 0.02565627 0.0235265  0.02028219 0.01536954\n",
      " 0.02978196 0.02590253 0.01971888 0.02398352 0.02692628 0.02478139\n",
      " 0.0538611  0.02031634 0.06033472 0.04549383 0.03095443 0.02428908\n",
      " 0.02627392 0.01848643 0.02716033 0.03188277 0.04137232 0.03405265\n",
      " 0.04221757 0.01953289 0.03240723 0.0139131  0.04461185 0.02181149\n",
      " 0.04297008 0.058527   0.05941943 0.01799412 0.03248042 0.03037186\n",
      " 0.05236129 0.02946471 0.03360285 0.02606386 0.11820947 0.03763593\n",
      " 0.02335644 0.04472107 0.04526589 0.02352444 0.01937784 0.02709365\n",
      " 0.02093952 0.01826484 0.03816805 0.02336171 0.07306545 0.01658561\n",
      " 0.05941552 0.02359625 0.04850118 0.03012599 0.0389113  0.06933519\n",
      " 0.07428351 0.02865796 0.04777349 0.02098213 0.01697511 0.02055665\n",
      " 0.0855272  0.03379426 0.03284284 0.07592837 0.07601656 0.01813371\n",
      " 0.05758328 0.02883584 0.05302093 0.0303304  0.01777357 0.04678937\n",
      " 0.18572014 0.03219287 0.02030204 0.02548523 0.02246442 0.02347553\n",
      " 0.02828433 0.01912436 0.06203143 0.04355449 0.01672526 0.04470587\n",
      " 0.02957128 0.03271028 0.02158752 0.03203059 0.0290484  0.02187272\n",
      " 0.0233156  0.04805988 0.02244181 0.02405277 0.04502417 0.03691395\n",
      " 0.03658802 0.10806727 0.03379345 0.02763877 0.06306051 0.03083965\n",
      " 0.02143852 0.03810965 0.02528727 0.04374532 0.08223817 0.04670918\n",
      " 0.02099934 0.02045179 0.01878328 0.02075599 0.01783452 0.03822418\n",
      " 0.02119106 0.02295714 0.02120765 0.06099292 0.0713616  0.02656464\n",
      " 0.02213486 0.05324619 0.03157377 0.04574856 0.01541575 0.02291742\n",
      " 0.03707619 0.03204233 0.03878222 0.01776559 0.02329564 0.03445041\n",
      " 0.02185821 0.01672826 0.02450362 0.02834421 0.03494228 0.04395726\n",
      " 0.01857674 0.0826736  0.0674182  0.05443645 0.02139309 0.01496023\n",
      " 0.02523341 0.31461371 0.01831001 0.10525583 0.03353459 0.01629151\n",
      " 0.04503944 0.03270669 0.01864643 0.06237209 0.0260338  0.04409366\n",
      " 0.01745576 0.02769636 0.09664558 0.0791564  0.01718069 0.02217254\n",
      " 0.04127898 0.02205592 0.03149922 0.03801714 0.02920368 0.06485188\n",
      " 0.02381446 0.02218942 0.01794434 0.04522342 0.04547267 0.03801795\n",
      " 0.03099925 0.03756891 0.04025938 0.02134622 0.04114056 0.0352718\n",
      " 0.02655775 0.02557855 0.02707875 0.04014994 0.03335504 0.02026999\n",
      " 0.03094266 0.02386561 0.03627436 0.0384655  0.01767854 0.01541722\n",
      " 0.01716095 0.04089244 0.04480407 0.0300905  0.06507072 0.02079896\n",
      " 0.04007723 0.02251899 0.02702305 0.02243279 0.02362851 0.04563871\n",
      " 0.02399136 0.02290872 0.01860195 0.05799892 0.03153779 0.02512541\n",
      " 0.11520946 0.01612064 0.01792772 0.01951281 0.02384142 0.02211278\n",
      " 0.0312631  0.02493634 0.03043671 0.05985831 0.01560169 0.02907326\n",
      " 0.020173   0.0372427  0.01683097 0.01715888 0.03938258 0.02457937\n",
      " 0.02374768 0.0252628  0.02314545 0.10370906 0.036905   0.0172011\n",
      " 0.0189213  0.0808855  0.05670252 0.03130496 0.04092133 0.03638097\n",
      " 0.0248225  0.02581969 0.02073205 0.02170812 0.04087618 0.02802944\n",
      " 0.02661909 0.03439859 0.01998021 0.03130535 0.03156665 0.01842463\n",
      " 0.0468325  0.02039884 0.01780554 0.03840193 0.0180926  0.03110906\n",
      " 0.05731642 0.04798078 0.04551606 0.02383463 0.03471627 0.0201911\n",
      " 0.02017632 0.0206931  0.04733142 0.01618381 0.02217208 0.0361906\n",
      " 0.03807021 0.03242037 0.05340061 0.03290082 0.04274793 0.02575426\n",
      " 0.02504587 0.02916122 0.03879699 0.03204809 0.01623213 0.0249867\n",
      " 0.02075078 0.08828302 0.02220919 0.03893297 0.03214723 0.04103937\n",
      " 0.02386329 0.03602611 0.02051435 0.0346221  0.02444791 0.04178744\n",
      " 0.03532117 0.03960632 0.19236554 0.04355122 0.02365379 0.03590651\n",
      " 0.02549283 0.03763627 0.0267576  0.04143479 0.04114099 0.03389544\n",
      " 0.02275848 0.03475957 0.04602811 0.0483842  0.02465322 0.03734087\n",
      " 0.01772509 0.03350661 0.03567445 0.02413539 0.04066314 0.02749781\n",
      " 0.16365663 0.04104039 0.03067934 0.01867505 0.01815276 0.04591384\n",
      " 0.03580977 0.02740823 0.02885461 0.02782783 0.03224895 0.01628358\n",
      " 0.02149063 0.02800451 0.04384674 0.01620348 0.05329388 0.01908808\n",
      " 0.01620118 0.0248562  0.03884126 0.02005191 0.07229996 0.05742548\n",
      " 0.02888776 0.04921148 0.03921377 0.03070097 0.01486764 0.06894266\n",
      " 0.02666373 0.05532716 0.04610498 0.02011742 0.06399107 0.02610805\n",
      " 0.01719931 0.01774079 0.03088434 0.02358609 0.02139336 0.02305702\n",
      " 0.03317455 0.0164095  0.03125911 0.02531198 0.04062954 0.0245251\n",
      " 0.03475131 0.02561007 0.03985617 0.04295898 0.03728636 0.0221081\n",
      " 0.02334834 0.0238822  0.02480743 0.02973053 0.02807303 0.06117306\n",
      " 0.02529058 0.02256114 0.03384543 0.02600337 0.03124291 0.01656964\n",
      " 0.04889689 0.02194998 0.02199224 0.02752915 0.02063892 0.05635928\n",
      " 0.01499377 0.03069057 0.0272489  0.01722033 0.02709871 0.02996182\n",
      " 0.03313535 0.0316947  0.02081368 0.03611292 0.04648224 0.02030197\n",
      " 0.03074879 0.03310634 0.0314736  0.11466113 0.03053944 0.02270961\n",
      " 0.03004506 0.0229585 ]\n"
     ]
    }
   ],
   "source": [
    "#m = Chem.MolFromSmiles('c1ccccc1')\n",
    "#original_graph =  molToGraph(m, molecular_attributes = True).dump_as_tensor()\n",
    "#G = GraphFP(output_dim =512 , inner_dim =39)\n",
    "#G.build()\n",
    "#fp = G.get_output_singlesample(original_graph)\n",
    "# print(fp.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "print(fp.eval().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# depth for 考虑相邻的原子，跟新原子属性"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
