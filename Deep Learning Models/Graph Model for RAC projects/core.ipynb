{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from saving import save_model_history, save_model_history_manual\n",
    "from neural_fp import sizeAttributeVectors, sizeAttributeVector\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Activation, Input, merge\n",
    "from keras.layers.core import Flatten, Permute, Reshape, Dropout, Lambda\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.optimizers import *\n",
    "# from keras.utils.visualize_util import plot\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import keras.backend as K\n",
    "import theano.tensor as T\n",
    "from GraphFP_embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.is_nan = T.isnan \n",
    "K.logical_not = lambda x: 1 - x\n",
    "\n",
    "\n",
    "def mse_no_NaN(y_true, y_pred):\n",
    "    '''For each sample, sum squared error ignoring NaN values'''\n",
    "    return K.sum(K.square(K.switch(K.logical_not(K.is_nan(y_true)), y_true, y_pred) - y_pred), axis = -1)\n",
    "\n",
    "def binary_crossnetropy_no_NaN(y_true, y_pred):\n",
    "    return K.sum(K.binary_crossentropy(K.switch(K.is_nan(y_true), y_pred, y_true), y_pred), axis = -1)\n",
    "\n",
    "def build_model(embedding_size = 512, lr = 0.01, optimizer = 'adam', depth = 2, scale_output = 0.05, \n",
    "                padding = True, hidden = 0, hidden2 = 0, loss = 'mse', hidden_activation = 'tanh',output_activation = 'softmax', \n",
    "                dr1 = 0.0, dr2 = 0.0, output_size = 1, sum_after = False,molecular_attributes = True, use_fp = None, inner_rep = 32):\n",
    "    \n",
    "    '''Generates simple embedding model to use molecular tensor as input in order to predict a single-valued output (i.e., yield)\n",
    "    inputs:\n",
    "        embedding_size - size of fingerprint for GraphFP layer\n",
    "        lr - learning rate to use (train_model overwrites this value)\n",
    "        optimizer - optimization function to use\n",
    "        depth - depth of the neural fingerprint (i.e., radius)\n",
    "        scale_output - initial scale for output weights in GraphFP\n",
    "        padding - whether or not molecular tensors will be padded (i.e., batch_size > 1)\n",
    "        hidden - number of hidden tanh nodes after FP (0 is linear)\n",
    "        hidden2 - number of hidden nodes after \"hidden\" layer\n",
    "        hidden_activation - activation function used in hidden layers\n",
    "        output_activation - activation function for final output nodes\n",
    "        dr1 - dropout rate after embedding\n",
    "        dr2 - dropout rate after hidden\n",
    "        loss - loss function as a string (e.g., 'mse')\n",
    "        sum_after - whether to sum neighbor contributions after passing\n",
    "                    through a single network layer, or to do so before\n",
    "                    passing them from the network layer (during updates)\n",
    "        molecular_attributes - whether to include additional molecular \n",
    "                    attributes in the atom-level features (recommended)\n",
    "        use_fp - whether the representation used is actually a fingerprint\n",
    "                    and not a convolutional network (for benchmarking)\n",
    "    outputs:\n",
    "        model - a Keras model'''\n",
    "\n",
    "\n",
    "\n",
    "    # Base model\n",
    "    if type(use_fp) == type(None):\n",
    "        \n",
    "        F = sizeAttributeVector(molecular_attributes = molecular_attributes) - 1\n",
    "        \n",
    "        #mat_features = Input(shape = (None, F_atom), name = \"feature matrix\")\n",
    "        #mat_adjacency = Input(shape = (None, None), name = \"adjacency/self matrix\")\n",
    "        #mat_specialbondtypes = Input(shape = (None, F_bond), name = \"special bond types\")\n",
    "        input_features = Input(shape = (None, None, F), name = \"feature graph\")\n",
    "        ecfps = Input(shape = (512,), name = \"ecfps\")\n",
    "        \n",
    "        FPS = []\n",
    "        \n",
    "        \n",
    "        nn_fps = GraphFP(output_dim = embedding_size, inner_dim = sizeAttributeVector(molecular_attributes) - 1, \n",
    "                         depth = depth,scale_output = scale_output,\n",
    "                  padding = padding, activation_inner = 'tanh')(input_features)\n",
    "\n",
    "        print('    model: added GraphFP layer ({} -> {})'.format('mol_a & mol_e', \"512\"))\n",
    "        \n",
    "        FPS.append(ecfps)\n",
    "        FPS.append(nn_fps)\n",
    "        \n",
    "        FPs = merge(FPS, mode = 'concat', concat_axis= 1, name = 'mix fps')\n",
    "\n",
    "        \n",
    "        if hidden > 0:\n",
    "            h1 = Dense(hidden, activation = hidden_activation)(FPs)\n",
    "            h1d = Dropout(dr1)(h1)\n",
    "            if verbose: print('    model: added {} Dense layer (-> {})'.format(hidden_activation, hidden))\n",
    "            if hidden2 > 0:\n",
    "                h2 = Dense(hidden2, activation = hidden_activation)(h1)\n",
    "                if verbose: print('    model: added {} Dense layer (-> {})'.format(hidden_activation, hidden2))\n",
    "                h = Dropout(dr2)(h2)\n",
    "            else:\n",
    "                h = h1d\n",
    "        else:\n",
    "            h = FPs\n",
    "\n",
    "    ypred = Dense(output_size, activation = output_activation)(h)\n",
    "    print('    model: added output Dense layer (-> {})'.format(output_size))\n",
    "\n",
    "\n",
    "\n",
    "    if type(use_fp) == type(None):\n",
    "        model = Model(input = [input_features, ecfps], \n",
    "            output = [ypred])\n",
    "    else:\n",
    "        model = Model(input = [FPs], \n",
    "            output = [ypred])\n",
    "\n",
    "    model.summary()\n",
    "    model.layers\n",
    "\n",
    "    # Compile\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(lr = lr)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(lr = lr)\n",
    "    elif optimizer == 'adagrad':\n",
    "        optimizer = Adagrad(lr = lr)\n",
    "    elif optimizer == 'adadelta':\n",
    "        optimizer = Adadelta()\n",
    "    else:\n",
    "        print('Unrecognized optimizer')\n",
    "        quit(1)\n",
    "\n",
    "    # Custom loss to filter out NaN values in multi-task predictions\n",
    "    if loss == 'custom':\n",
    "        loss = mse_no_NaN\n",
    "    elif loss == 'custom2':\n",
    "        loss = binary_crossnetropy_no_NaN\n",
    "\n",
    "    print('compiling...',)\n",
    "    model.compile(loss = loss, optimizer = optimizer)\n",
    "    print('done')\n",
    "\n",
    "    return model\n",
    "\n",
    "def save_model(model, loss, val_loss, fpath = '', config = {}, tstamp = ''):\n",
    "    '''Saves NN model object and associated information.\n",
    "    inputs:\n",
    "        model - a Keras model\n",
    "        loss - list of training losses \n",
    "        val_loss - list of validation losses\n",
    "        fpath - root filepath to save everything to (with .json, h5, png, info)\n",
    "        config - the configuration dictionary that defined this model \n",
    "        tstamp - current timestamp to log in info file'''\n",
    "\n",
    "    # Dump data\n",
    "    with open(fpath + '.json', 'w') as structure_fpath:\n",
    "        json.dump(model.to_json(), structure_fpath)\n",
    "    print('...saved structural information')\n",
    "\n",
    "    # Dump weights\n",
    "    model.save_weights(fpath + '.h5', overwrite = True)\n",
    "    print('...saved weights')\n",
    "\n",
    "    # # Dump image\n",
    "    # plot(model, to_file = fpath + '.png')\n",
    "    # print('...saved image')\n",
    "\n",
    "    # Dump history\n",
    "    save_model_history_manual(loss, val_loss, fpath + '.hist')\n",
    "    print ('...saved history')\n",
    "\n",
    "    # Write to info file\n",
    "    info_fid = open(fpath + '.info', 'a')\n",
    "    info_fid.write('{} saved {}\\n\\n'.format(fpath, tstamp))\n",
    "    info_fid.write('Configuration details\\n------------\\n')\n",
    "    info_fid.write('  {}\\n'.format(config))\n",
    "    info_fid.close()\n",
    "\n",
    "    print('...saved model to {}.[json, h5, png, info]'.format(fpath))\n",
    "    return True\n",
    "\n",
    "\n",
    "def train_model(model, data, nb_epoch = 0, batch_size = 1, lr_func = None, patience = 10, verbose = False):\n",
    "    '''Trains the model.\n",
    "    inputs:\n",
    "        model - a Keras model\n",
    "        data - three dictionaries for training,\n",
    "                validation, and testing separately\n",
    "        nb_epoch - number of epochs to train for\n",
    "        batch_size - batch_size to use on the data. This must agree with what was\n",
    "                specified for data (i.e., if tensors are padded or not)\n",
    "        lr_func - string which is evaluated with 'epoch' to produce the learning \n",
    "                rate at each epoch \n",
    "        patience - number of epochs to wait when no progress is being made in \n",
    "                the validation loss. a patience of -1 means that the model will\n",
    "                use weights from the best-performing model during training\n",
    "    outputs:\n",
    "        model - a trained Keras model\n",
    "        loss - list of training losses corresponding to each epoch \n",
    "        val_loss - list of validation losses corresponding to each epoch'''\n",
    "\n",
    "    # Unpack data \n",
    "    (train, val, test) = data\n",
    "    mols_train = train['mols']; y_train = train['y']; smiles_train = train['smiles']\n",
    "    mols_val   = val['mols'];   y_val   = val['y'];   smiles_val   = val['smiles']\n",
    "    print('{} to train on'.format(len(mols_train)))\n",
    "    print('{} to validate on'.format(len(mols_val)))\n",
    "    print('{} to test on'.format(len(smiles_val)))\n",
    "\n",
    "    # Create learning rate function\n",
    "    if lr_func:\n",
    "        lr_func_string = 'def lr(epoch):\\n    return {}\\n'.format(lr_func)\n",
    "        exec (lr_func_string)\n",
    "\n",
    "\n",
    "    # Fit (allows keyboard interrupts in the middle)\n",
    "    # Because molecular graph tensors are different sizes based on N_atoms, can only do one at a time\n",
    "    # (alternative is to pad with zeros and try to add some masking feature to GraphFP)\n",
    "    # -> this is why batch_size == 1 is treated distinctly\n",
    "    try:\n",
    "        loss = []\n",
    "        val_loss = []\n",
    "\n",
    "        if batch_size == 1: # DO NOT NEED TO PAD\n",
    "            wait = 0\n",
    "            prev_best_val_loss = 99999999\n",
    "            for i in range(nb_epoch):\n",
    "                this_loss = []\n",
    "                this_val_loss = []\n",
    "                if lr_func: model.optimizer.lr.set_value(lr(i))\n",
    "                print('Epoch {}/{}, lr = {}'.format(i + 1, nb_epoch, model.optimizer.lr.get_value()))\n",
    "\n",
    "                # Run through training set\n",
    "                print('Training...')\n",
    "                training_order = range(len(mols_train))\n",
    "                np.random.shuffle(training_order)\n",
    "                for j in training_order:\n",
    "                    single_mol = mols_train[j]\n",
    "                    single_y_as_array = np.reshape(y_train[j], (1, -1))\n",
    "                    sloss = model.train_on_batch(\n",
    "                        [np.array([single_mol][0:3]), np.array([single_mol[3]])],\n",
    "                        single_y_as_array\n",
    "                    )\n",
    "                    this_loss.append(sloss)\n",
    "\n",
    "                # Run through testing set\n",
    "                print('Validating..')\n",
    "                for j in range(len(mols_val)):\n",
    "                    single_mol = mols_val[j]\n",
    "                    single_y_as_array = np.reshape(y_val[j], (1, -1))\n",
    "                    sloss = model.test_on_batch(\n",
    "                        [np.array([single_mol][0:3]), np.array([single_mol[3]])],\n",
    "                        single_y_as_array\n",
    "                    )\n",
    "                    this_val_loss.append(sloss)\n",
    "\n",
    "                loss.append(np.mean(this_loss))\n",
    "                val_loss.append(np.mean(this_val_loss))\n",
    "                print('loss: {}\\tval_loss: {}'.format(loss[i], val_loss[i]))\n",
    "\n",
    "                # Check progress\n",
    "                if np.mean(this_val_loss) < prev_best_val_loss:\n",
    "                    wait = 0\n",
    "                    prev_best_val_loss = np.mean(this_val_loss)\n",
    "                    if patience == -1:\n",
    "                        model.save_weights('best.h5', overwrite=True)\n",
    "                else:\n",
    "                    wait = wait + 1\n",
    "                    print('{} epochs without val_loss progress'.format(wait))\n",
    "                    if wait == patience:\n",
    "                        print('stopping early!')\n",
    "                        break\n",
    "            if patience == -1:\n",
    "                model.load_weights('best.h5')\n",
    "\n",
    "        else: \n",
    "            # When the batch_size is larger than one, we have padded mol tensors\n",
    "            # which  means we need to concatenate them but can use Keras' built-in\n",
    "            # training functions with callbacks, validation_split, etc.\n",
    "            if lr_func:\n",
    "                callbacks = [LearningRateScheduler(lr)]\n",
    "            else:\n",
    "                callbacks = []\n",
    "            if patience != -1:\n",
    "                callbacks.append(EarlyStopping(patience = patience, verbose = 1))\n",
    "\n",
    "            if mols_val:\n",
    "                mols = np.vstack((mols_train, mols_val))\n",
    "                y = np.concatenate((y_train, y_val))\n",
    "                hist = model.fit(mols, y, \n",
    "                    nb_epoch = nb_epoch, \n",
    "                    batch_size = batch_size, \n",
    "                    validation_split = (1 - float(len(mols_train))/(len(mols_val) + len(mols_train))),\n",
    "                    verbose = verbose,\n",
    "                    callbacks = callbacks)\t\n",
    "            else:\n",
    "                hist = model.fit(np.array(mols_train), np.array(y_train), \n",
    "                    nb_epoch = nb_epoch, \n",
    "                    batch_size = batch_size, \n",
    "                    verbose = verbose,\n",
    "                    callbacks = callbacks)\n",
    "\n",
    "            loss = []; val_loss = []\n",
    "            if 'loss' in hist.history: loss = hist.history['loss']\n",
    "            if 'val_loss' in hist.history: val_loss = hist.history['val_loss']\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('User terminated training early (intentionally)')\n",
    "\n",
    "    return (model, loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"build_model(embedding_size = 512, lr = 0.01, optimizer = 'adam', depth = 2, scale_output = 0.05, \\n                padding = False, hidden = 0, hidden2 = 0, loss = 'mse', hidden_activation = 'tanh',output_activation = 'softmax', \\n                dr1 = 0.0, dr2 = 0.0, output_size = 1, sum_after = False,molecular_attributes = True, use_fp = None, inner_rep = 32)\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"build_model(embedding_size = 512, lr = 0.01, optimizer = 'adam', depth = 2, scale_output = 0.05, \n",
    "                padding = False, hidden = 0, hidden2 = 0, loss = 'mse', hidden_activation = 'tanh',output_activation = 'softmax', \n",
    "                dr1 = 0.0, dr2 = 0.0, output_size = 1, sum_after = False,molecular_attributes = True, use_fp = None, inner_rep = 32)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
