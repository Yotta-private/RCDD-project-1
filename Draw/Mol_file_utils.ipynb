{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from utils import Variable\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"A class for handling encoding/decoding from SMILES to an array of indices\"\"\"\n",
    "    def __init__(self, init_from_file=None, max_length=140):\n",
    "        self.special_tokens = ['EOS', 'GO']\n",
    "        self.additional_chars = set()\n",
    "        self.chars = self.special_tokens\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.reversed_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        self.max_length = max_length\n",
    "        if init_from_file: self.init_from_file(init_from_file)\n",
    "\n",
    "    def encode(self, char_list):\n",
    "        \"\"\"Takes a list of characters (eg '[NH]') and encodes to array of indices\"\"\"\n",
    "        smiles_matrix = np.zeros(len(char_list), dtype=np.float32)\n",
    "        for i, char in enumerate(char_list):\n",
    "            smiles_matrix[i] = self.vocab[char]\n",
    "        return smiles_matrix\n",
    "\n",
    "    def decode(self, matrix):\n",
    "        \"\"\"Takes an array of indices and returns the corresponding SMILES\"\"\"\n",
    "        chars = []\n",
    "        for i in matrix:\n",
    "            if i == self.vocab['EOS']: break\n",
    "            chars.append(self.reversed_vocab[i])\n",
    "        smiles = \"\".join(chars)\n",
    "        smiles = smiles.replace(\"L\", \"Cl\").replace(\"R\", \"Br\")\n",
    "        return smiles\n",
    "\n",
    "    def tokenize(self, smiles):\n",
    "        \"\"\"Takes a SMILES and return a list of characters/tokens\"\"\"\n",
    "        regex = '(\\[[^\\[\\]]{1,10}\\])'\n",
    "        smiles = replace_halogen(smiles)\n",
    "        char_list = re.split(regex, smiles)\n",
    "        tokenized = []\n",
    "        for char in char_list:\n",
    "            if char.startswith('['):\n",
    "                tokenized.append(char)\n",
    "            else:\n",
    "                chars = [unit for unit in char]\n",
    "                [tokenized.append(unit) for unit in chars]\n",
    "        tokenized.append('EOS')\n",
    "        return tokenized\n",
    "\n",
    "    def add_characters(self, chars):\n",
    "        \"\"\"Adds characters to the vocabulary\"\"\"\n",
    "        for char in chars:\n",
    "            self.additional_chars.add(char)\n",
    "        char_list = list(self.additional_chars)\n",
    "        char_list.sort()\n",
    "        self.chars = char_list + self.special_tokens\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.reversed_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def init_from_file(self, file):\n",
    "        \"\"\"Takes a file containing \\n separated characters to initialize the vocabulary\"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            chars = f.read().split()\n",
    "        self.add_characters(chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chars)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Vocabulary containing {} tokens: {}\".format(len(self), self.chars)\n",
    "\n",
    "class MolData(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset that takes a file containing SMILES.\n",
    "\n",
    "        Args:\n",
    "                fname : path to a file containing \\n separated SMILES.\n",
    "                voc   : a Vocabulary instance\n",
    "\n",
    "        Returns:\n",
    "                A custom PyTorch dataset for training the Prior.\n",
    "    \"\"\"\n",
    "    def __init__(self, fname, voc):\n",
    "        self.voc = voc\n",
    "        self.smiles = []\n",
    "        with open(fname, 'r') as f:\n",
    "            for line in f:\n",
    "                self.smiles.append(line.split()[0])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        mol = self.smiles[i]\n",
    "        tokenized = self.voc.tokenize(mol)\n",
    "        encoded = self.voc.encode(tokenized)\n",
    "        return Variable(encoded)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Dataset containing {} structures.\".format(len(self))\n",
    "\n",
    "    @classmethod\n",
    "    def collate_fn(cls, arr):\n",
    "        \"\"\"Function to take a list of encoded sequences and turn them into a batch\"\"\"\n",
    "        max_length = max([seq.size(0) for seq in arr])\n",
    "        collated_arr = Variable(torch.zeros(len(arr), max_length))\n",
    "        for i, seq in enumerate(arr):\n",
    "            collated_arr[i, :seq.size(0)] = seq\n",
    "        return collated_arr\n",
    "\n",
    "class Experience(object):\n",
    "    \"\"\"Class for prioritized experience replay that remembers the highest scored sequences\n",
    "       seen and samples from them with probabilities relative to their scores.\"\"\"\n",
    "    def __init__(self, voc, max_size=100):\n",
    "        self.memory = []\n",
    "        self.max_size = max_size\n",
    "        self.voc = voc\n",
    "\n",
    "    def add_experience(self, experience):\n",
    "        \"\"\"Experience should be a list of (smiles, score, prior likelihood) tuples\"\"\"\n",
    "        self.memory.extend(experience)\n",
    "        if len(self.memory)>self.max_size:\n",
    "            # Remove duplicates\n",
    "            idxs, smiles = [], []\n",
    "            for i, exp in enumerate(self.memory):\n",
    "                if exp[0] not in smiles:\n",
    "                    idxs.append(i)\n",
    "                    smiles.append(exp[0])\n",
    "            self.memory = [self.memory[idx] for idx in idxs]\n",
    "            # Retain highest scores\n",
    "            self.memory.sort(key = lambda x: x[1], reverse=True)\n",
    "            self.memory = self.memory[:self.max_size]\n",
    "            print(\"\\nBest score in memory: {:.2f}\".format(self.memory[0][1]))\n",
    "\n",
    "    def sample(self, n):\n",
    "        \"\"\"Sample a batch size n of experience\"\"\"\n",
    "        if len(self.memory)<n:\n",
    "            raise IndexError('Size of memory ({}) is less than requested sample ({})'.format(len(self), n))\n",
    "        else:\n",
    "            scores = [x[1] for x in self.memory]\n",
    "            sample = np.random.choice(len(self), size=n, replace=False, p=scores/np.sum(scores))\n",
    "            sample = [self.memory[i] for i in sample]\n",
    "            smiles = [x[0] for x in sample]\n",
    "            scores = [x[1] for x in sample]\n",
    "            prior_likelihood = [x[2] for x in sample]\n",
    "        tokenized = [self.voc.tokenize(smile) for smile in smiles]\n",
    "        encoded = [Variable(self.voc.encode(tokenized_i)) for tokenized_i in tokenized]\n",
    "        encoded = MolData.collate_fn(encoded)\n",
    "        return encoded, np.array(scores), np.array(prior_likelihood)\n",
    "\n",
    "    def initiate_from_file(self, fname, scoring_function, Prior):\n",
    "        \"\"\"Adds experience from a file with SMILES\n",
    "           Needs a scoring function and an RNN to score the sequences.\n",
    "           Using this feature means that the learning can be very biased\n",
    "           and is typically advised against.\"\"\"\n",
    "        with open(fname, 'r') as f:\n",
    "            smiles = []\n",
    "            for line in f:\n",
    "                smile = line.split()[0]\n",
    "                if Chem.MolFromSmiles(smile):\n",
    "                    smiles.append(smile)\n",
    "        scores = scoring_function(smiles)\n",
    "        tokenized = [self.voc.tokenize(smile) for smile in smiles]\n",
    "        encoded = [Variable(self.voc.encode(tokenized_i)) for tokenized_i in tokenized]\n",
    "        encoded = MolData.collate_fn(encoded)\n",
    "        prior_likelihood, _ = Prior.likelihood(encoded.long())\n",
    "        prior_likelihood = prior_likelihood.data.cpu().numpy()\n",
    "        new_experience = zip(smiles, scores, prior_likelihood)\n",
    "        self.add_experience(new_experience)\n",
    "\n",
    "    def print_memory(self, path):\n",
    "        \"\"\"Prints the memory.\"\"\"\n",
    "        print(\"\\n\" + \"*\" * 80 + \"\\n\")\n",
    "        print(\"         Best recorded SMILES: \\n\")\n",
    "        print(\"Score     Prior log P     SMILES\\n\")\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(\"SMILES Score PriorLogP\\n\")\n",
    "            for i, exp in enumerate(self.memory[:100]):\n",
    "                if i < 50:\n",
    "                    print(\"{:4.2f}   {:6.2f}        {}\".format(exp[1], exp[2], exp[0]))\n",
    "                    f.write(\"{} {:4.2f} {:6.2f}\\n\".format(*exp))\n",
    "        print(\"\\n\" + \"*\" * 80 + \"\\n\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "def replace_halogen(string):\n",
    "    \"\"\"Regex to replace Br and Cl with single letters\"\"\"\n",
    "    br = re.compile('Br')\n",
    "    cl = re.compile('Cl')\n",
    "    string = br.sub('R', string)\n",
    "    string = cl.sub('L', string)\n",
    "\n",
    "    return string\n",
    "\n",
    "def tokenize(smiles):\n",
    "    \"\"\"Takes a SMILES string and returns a list of tokens.\n",
    "    This will swap 'Cl' and 'Br' to 'L' and 'R' and treat\n",
    "    '[xx]' as one token.\"\"\"\n",
    "    regex = '(\\[[^\\[\\]]{1,10}\\])'\n",
    "    smiles = replace_halogen(smiles)\n",
    "    char_list = re.split(regex, smiles)\n",
    "    tokenized = []\n",
    "    for char in char_list:\n",
    "        if char.startswith('['):\n",
    "            tokenized.append(char)\n",
    "        else:\n",
    "            chars = [unit for unit in char]\n",
    "            [tokenized.append(unit) for unit in chars]\n",
    "    tokenized.append('EOS')\n",
    "    return tokenized\n",
    "\n",
    "def canonicalize_smiles_from_file(fname):\n",
    "    \"\"\"Reads a SMILES file and returns a list of RDKIT SMILES\"\"\"\n",
    "    with open(fname, 'r') as f:\n",
    "        smiles_list = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i % 100000 == 0:\n",
    "                print(\"{} lines processed.\".format(i))\n",
    "            smiles = line.split(\" \")[0]\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if filter_mol(mol):\n",
    "                smiles_list.append(Chem.MolToSmiles(mol,isomericSmiles=True))\n",
    "        print(\"{} SMILES retrieved\".format(len(smiles_list)))\n",
    "        return smiles_list\n",
    "\n",
    "def filter_mol(mol, max_heavy_atoms=100, min_heavy_atoms=10, element_list=[6,7,8,9,16,17,35]):\n",
    "    \"\"\"Filters molecules on number of heavy atoms and atom types\"\"\"\n",
    "    if mol is not None:\n",
    "        num_heavy = min_heavy_atoms<mol.GetNumHeavyAtoms()<max_heavy_atoms\n",
    "        elements = all([atom.GetAtomicNum() in element_list for atom in mol.GetAtoms()])\n",
    "        if num_heavy and elements:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def write_smiles_to_file(smiles_list, fname):\n",
    "    \"\"\"Write a list of SMILES to a file.\"\"\"\n",
    "    with open(fname, 'w') as f:\n",
    "        for smiles in smiles_list:\n",
    "            f.write(smiles + \"\\n\")\n",
    "\n",
    "def filter_on_chars(smiles_list, chars):\n",
    "    \"\"\"Filters SMILES on the characters they contain.\n",
    "       Used to remove SMILES containing very rare/undesirable\n",
    "       characters.\"\"\"\n",
    "    smiles_list_valid = []\n",
    "    for smiles in smiles_list:\n",
    "        tokenized = tokenize(smiles)\n",
    "        if all([char in chars for char in tokenized][:-1]):\n",
    "            smiles_list_valid.append(smiles)\n",
    "    return smiles_list_valid\n",
    "\n",
    "def filter_file_on_chars(smiles_fname, voc_fname):\n",
    "    \"\"\"Filters a SMILES file using a vocabulary file.\n",
    "       Only SMILES containing nothing but the characters\n",
    "       in the vocabulary will be retained.\"\"\"\n",
    "    smiles = []\n",
    "    with open(smiles_fname, 'r') as f:\n",
    "        for line in f:\n",
    "            smiles.append(line.split()[0])\n",
    "    print(smiles[:10])\n",
    "    chars = []\n",
    "    with open(voc_fname, 'r') as f:\n",
    "        for line in f:\n",
    "            chars.append(line.split()[0])\n",
    "    print(chars)\n",
    "    valid_smiles = filter_on_chars(smiles, chars)\n",
    "    with open(smiles_fname + \"_filtered\", 'w') as f:\n",
    "        for smiles in valid_smiles:\n",
    "            f.write(smiles + \"\\n\")\n",
    "\n",
    "def combine_voc_from_files(fnames):\n",
    "    \"\"\"Combine two vocabularies\"\"\"\n",
    "    chars = set()\n",
    "    for fname in fnames:\n",
    "        with open(fname, 'r') as f:\n",
    "            for line in f:\n",
    "                chars.add(line.split()[0])\n",
    "    print(len(chars))\n",
    "    with open(\"_\".join(fnames) + '_combined', 'w') as f:\n",
    "        for char in chars:\n",
    "            f.write(char + \"\\n\")\n",
    "\n",
    "def construct_vocabulary(smiles_list):\n",
    "    \"\"\"Returns all the characters present in a SMILES file.\n",
    "       Uses regex to find characters/tokens of the format '[x]'.\"\"\"\n",
    "    add_chars = set()\n",
    "    for i, smiles in enumerate(smiles_list):\n",
    "        regex = '(\\[[^\\[\\]]{1,10}\\])'\n",
    "        smiles = replace_halogen(smiles)\n",
    "        char_list = re.split(regex, smiles)\n",
    "        for char in char_list:\n",
    "            if char.startswith('['):\n",
    "                add_chars.add(char)\n",
    "            else:\n",
    "                chars = [unit for unit in char]\n",
    "                [add_chars.add(unit) for unit in chars]\n",
    "\n",
    "    print(\"Number of characters: {}\".format(len(add_chars)))\n",
    "    with open('data/p2014_voc', 'w') as f:\n",
    "        for char in add_chars:\n",
    "            f.write(char + \"\\n\")\n",
    "    return add_chars\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading smiles...\n",
      "0 lines processed.\n",
      "100000 lines processed.\n",
      "153732 SMILES retrieved\n",
      "Constructing vocabulary...\n",
      "Number of characters: 87\n"
     ]
    }
   ],
   "source": [
    "smiles_file = \"./data/biogenic.txt\"\n",
    "print(\"Reading smiles...\")\n",
    "smiles_list = canonicalize_smiles_from_file(smiles_file)\n",
    "print(\"Constructing vocabulary...\")\n",
    "voc_chars = construct_vocabulary(smiles_list)\n",
    "write_smiles_to_file(smiles_list, \"data/biogenic_filtered.smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading smiles...\n",
      "0 lines processed.\n",
      "2933 SMILES retrieved\n",
      "Constructing vocabulary...\n",
      "Number of characters: 46\n"
     ]
    }
   ],
   "source": [
    "smiles_file = \"./data/p2013.smi\"\n",
    "print(\"Reading smiles...\")\n",
    "smiles_list = canonicalize_smiles_from_file(smiles_file)\n",
    "print(\"Constructing vocabulary...\")\n",
    "voc_chars = construct_vocabulary(smiles_list)\n",
    "write_smiles_to_file(smiles_list, \"data/p2013_filtered.smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading smiles...\n",
      "0 lines processed.\n",
      "3180 SMILES retrieved\n",
      "Constructing vocabulary...\n",
      "Number of characters: 49\n"
     ]
    }
   ],
   "source": [
    "smiles_file = \"./data/p2014-2015.smi\"\n",
    "print(\"Reading smiles...\")\n",
    "smiles_list = canonicalize_smiles_from_file(smiles_file)\n",
    "print(\"Constructing vocabulary...\")\n",
    "voc_chars = construct_vocabulary(smiles_list)\n",
    "write_smiles_to_file(smiles_list, \"data/p2014_filtered.smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC[C@H]1C[C@@H]2C[C@@H]3C=CC=CC(=O)NCC[C@H](O)[C@@H]4NC(=O)C(=C(O)C=CC=C[C@H](O)[C@H]3[C@@H]2[C@H]1C)C4=O', 'CC[C@H]1C[C@@H]2C[C@@H]3C=CC=CC(=O)NCCC[C@@H]4NC(=O)C(=C(O)C=CC=C[C@H](O)[C@H]3[C@@H]2[C@H]1C)C4=O', 'CN[C@H](C(=O)O[C@@H]1C[C@H](C)N2C[C@]3(NC)C[C@]4(C(=O)Nc5c(C(=O)[C@H]6OC6(C)C)cccc54)C(C)(C)[C@@]3(O)C[C@H]2C1)C(C)C', 'CN[C@]12CN3[C@@H](C)C[C@@H](OC(=O)[C@@H](N)C(C)C)C[C@@H]3C[C@]1(O)C(C)(C)[C@]1(C2)C(=O)Nc2c(C(=O)[C@H]3OC3(C)C)cccc21', 'CN[C@]12CN3[C@@H](C)CCC[C@@H]3C[C@@H]1C(C)(C)[C@]1(C2)C(=O)Nc2c(C(=O)[C@H]3OC3(C)C)cccc21', 'N=C(N)NCCCCNC(=O)[C@H](Cc1ccccc1)NC(=O)C1OC1C(=O)O', 'CCCC[C@H](CC)C[C@]1(CC)C=C(CC)[C@H](CC(=O)OC)OO1', 'CCCCC(CC)C[C@]1(C)C=C(CC)[C@H](CC(=O)OC)OO1', 'CCCC[C@H](CC)C[C@@]1(CC)C[C@H](CC)[C@H](CC(=O)OC)OO1', 'CCCC[C@H](CC)C[C@@]1(CC)C[C@H](CC)[C@@H](CC(=O)OC)OO1']\n",
      "['o', '[N@+]', '[CH+]', '[S@]', 'N', '8', '4', '[CH2:10]', '[S+]', '[N]', '[C@:5]', '[cH-]', 'S', '[O]', '[C@@H:9]', '[CH]', '[s+]', 'n', '=', '[CH2:18]', '[N:19]', '[O:21]', '[CH2:16]', '%', 's', '[CH2:11]', '[O+]', '[S@@+]', '[CH2:12]', '[C@H:9]', '[c:4]', 'O', '[o+]', '3', '[c+]', '[CH2-]', '0', 'L', '7', '[C+]', '[O:7]', '[C:20]', '[N-]', '[C]', '[SH]', '[O-]', '#', '[cH:8]', '[CH3:14]', '[C@H]', ')', '[C@@]', '[C@@:20]', '[c:3]', '\\\\', '(', '[C-]', '[N@@+]', '[CH-]', '[NH:19]', '-', '[N+]', '6', 'R', 'C', '[C:17]', '[C@@:5]', '5', '[C@@H:15]', '[C@]', '[c:6]', '[CH:16]', '[nH]', '[C@@H]', '9', 'c', '2', '[cH:13]', '/', '1', '[n+]', '[S@@]', 'F', '[S@+]', '[c-]', '[c:2]', '[C@H:15]']\n"
     ]
    }
   ],
   "source": [
    "# combine_voc_from_files(['data/all_voc','data/p2013_voc','data/p2014_voc'])\n",
    "filter_file_on_chars('data/p2014_filtered.smi','data/all_voc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/biogenic_filtered.smi',encoding = 'utf-8')\n",
    "#df.drop_duplicates(keep='first', inplace=True) #去重\n",
    "df = df.sample(frac=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30746\n"
     ]
    }
   ],
   "source": [
    "cut_idx = int(round(0.2 * df.shape[0]))\n",
    "print(cut_idx)\n",
    "# df_test, df_train = df.iloc[:cut_idx], df.iloc[cut_idx:]\n",
    "df1, df2, df3, df4, df5 = df.iloc[:cut_idx],df.iloc[cut_idx:2*cut_idx],df.iloc[2*cut_idx:3*cut_idx],df.iloc[3*cut_idx:4*cut_idx],df.iloc[4*cut_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.to_csv(\"data/CV/cv_test1\",index=None)\n",
    "df2.to_csv(\"data/CV/cv_test2\",index=None)\n",
    "df3.to_csv(\"data/CV/cv_test3\",index=None)\n",
    "df4.to_csv(\"data/CV/cv_test4\",index=None)\n",
    "df5.to_csv(\"data/CV/cv_test5\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1_train = pd.concat([df2,df3,df4,df5],axis =0)\n",
    "df1_train.to_csv(\"data/CV/cv_train1\",index=None)\n",
    "\n",
    "df2_train = pd.concat([df1,df3,df4,df5],axis =0)\n",
    "df2_train.to_csv(\"data/CV/cv_train2\",index=None)\n",
    "\n",
    "df3_train = pd.concat([df1,df2,df4,df5],axis =0)\n",
    "df3_train.to_csv(\"data/CV/cv_train3\",index=None)\n",
    "\n",
    "df4_train = pd.concat([df1,df2,df3,df5],axis =0)\n",
    "df4_train.to_csv(\"data/CV/cv_train4\",index=None)\n",
    "\n",
    "df5_train = pd.concat([df2,df3,df4,df1],axis =0)\n",
    "df5_train.to_csv(\"data/CV/cv_train5\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
